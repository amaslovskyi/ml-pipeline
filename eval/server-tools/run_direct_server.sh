#!/bin/bash

# Direct inference server launcher (bypasses Ollama compatibility issues)

echo "ğŸš€ Starting Qwen DevOps Foundation Server"
echo "=========================================="

# Check if we're in the right directory
if [[ ! -f "direct_inference_server.py" ]]; then
    echo "âŒ direct_inference_server.py not found!"
    echo "ğŸ’¡ Make sure you're in the ml-pipeline/scripts directory"
    exit 1
fi

# Check Python dependencies
echo "ğŸ” Checking Python dependencies..."
python3 -c "
import sys
missing = []
try:
    import torch
    import transformers
    import peft
    import fastapi
    import uvicorn
    import pydantic
except ImportError as e:
    missing.append(str(e).split()[-1])

if missing:
    print(f'âŒ Missing dependencies: {missing}')
    print('ğŸ’¡ Install with: pip install torch transformers peft fastapi uvicorn pydantic')
    sys.exit(1)
else:
    print('âœ… All dependencies found')
"

if [[ $? -ne 0 ]]; then
    echo "ğŸ“¦ Installing missing dependencies..."
    pip install torch transformers peft fastapi uvicorn pydantic
fi

# Check for model files
echo "ğŸ” Checking for model files..."
if [[ -d "$HOME/Downloads/qwen-devops-model" ]]; then
    echo "âœ… Local model found at ~/Downloads/qwen-devops-model"
    LOCAL_MODEL=true
else
    echo "âš ï¸  Local model not found, will use HuggingFace Hub"
    LOCAL_MODEL=false
fi

# Check system resources
echo "ğŸ’¾ Checking system resources..."
python3 -c "
import psutil
ram_gb = psutil.virtual_memory().total / (1024**3)
available_gb = psutil.virtual_memory().available / (1024**3)
print(f'ğŸ’¾ Total RAM: {ram_gb:.1f} GB')
print(f'ğŸ’¾ Available RAM: {available_gb:.1f} GB')

if available_gb < 20:
    print('âš ï¸  WARNING: Less than 20GB RAM available')
    print('ğŸ’¡ Close other applications for better performance')
else:
    print('âœ… Sufficient RAM for model loading')
"

# Set environment variables
export PYTORCH_ENABLE_MPS_FALLBACK=1
export TOKENIZERS_PARALLELISM=false

echo ""
echo "ğŸ¯ Server Configuration:"
echo "------------------------"
echo "ğŸ“ URL: http://localhost:8000"
echo "ğŸ“– API Docs: http://localhost:8000/docs"
echo "ğŸ”§ Interactive API: http://localhost:8000/redoc"
echo ""

# Test endpoints that will be available
echo "ğŸ§ª Example API calls:"
echo "---------------------"
echo "Health check:"
echo "  curl http://localhost:8000/health"
echo ""
echo "Chat request:"
echo "  curl -X POST http://localhost:8000/chat \\"
echo "    -H 'Content-Type: application/json' \\"
echo "    -d '{\"message\": \"How do I deploy to Kubernetes?\"}'"
echo ""

# Start the server
echo "ğŸš€ Starting server..."
echo "Press Ctrl+C to stop"
echo ""

python3 direct_inference_server.py
