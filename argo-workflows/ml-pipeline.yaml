apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: ml-training-pipeline
  namespace: argo
spec:
  serviceAccountName: argo-workflow-sa
  entrypoint: ml-pipeline
  arguments:
    parameters:
      - name: model-name
        value: "bert-classifier"
      - name: data-version
        value: "v1.0"
      - name: epochs
        value: "3"
      - name: batch-size
        value: "16"
      - name: dvc-s3-bucket
        value: "mlops-data-bucket-1754159204"
      - name: dvc-s3-region
        value: "us-east-1"

  templates:
    - name: ml-pipeline
      dag:
        tasks:
          - name: data-preprocessing
            template: data-prep
            arguments:
              parameters:
                - name: data-version
                  value: "{{workflow.parameters.data-version}}"
                - name: dvc-s3-bucket
                  value: "{{workflow.parameters.dvc-s3-bucket}}"
                - name: dvc-s3-region
                  value: "{{workflow.parameters.dvc-s3-region}}"

          - name: feature-engineering
            template: feature-eng
            dependencies: [data-preprocessing]
            arguments:
              parameters:
                - name: model-name
                  value: "{{workflow.parameters.model-name}}"
                - name: data-version
                  value: "{{workflow.parameters.data-version}}"
                - name: dvc-s3-bucket
                  value: "{{workflow.parameters.dvc-s3-bucket}}"
                - name: dvc-s3-region
                  value: "{{workflow.parameters.dvc-s3-region}}"

          - name: model-training
            template: train-model
            dependencies: [feature-engineering]
            arguments:
              parameters:
                - name: model-name
                  value: "{{workflow.parameters.model-name}}"
                - name: epochs
                  value: "{{workflow.parameters.epochs}}"
                - name: batch-size
                  value: "{{workflow.parameters.batch-size}}"
                - name: dvc-s3-bucket
                  value: "{{workflow.parameters.dvc-s3-bucket}}"
                - name: dvc-s3-region
                  value: "{{workflow.parameters.dvc-s3-region}}"

          - name: model-evaluation
            template: evaluate-model
            dependencies: [model-training]
            arguments:
              parameters:
                - name: model-name
                  value: "{{workflow.parameters.model-name}}"

          - name: model-registration
            template: register-model
            dependencies: [model-evaluation]
            arguments:
              parameters:
                - name: model-name
                  value: "{{workflow.parameters.model-name}}"
                - name: dvc-s3-bucket
                  value: "{{workflow.parameters.dvc-s3-bucket}}"
                - name: dvc-s3-region
                  value: "{{workflow.parameters.dvc-s3-region}}"

    - name: data-prep
      inputs:
        parameters:
          - name: data-version
          - name: dvc-s3-bucket
          - name: dvc-s3-region
      container:
        image: python:3.9-slim
        command: [bash]
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: AWS_SECRET_ACCESS_KEY
          - name: AWS_DEFAULT_REGION
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: AWS_DEFAULT_REGION
        args:
          - -c
          - |
            echo "Starting data preprocessing for version {{inputs.parameters.data-version}}"
            # Fast installation with pinned versions to avoid dependency hell
            export PIP_DISABLE_PIP_VERSION_CHECK=1
            pip install --no-cache-dir pandas==2.3.1 numpy==2.0.2 scikit-learn==1.6.1 boto3==1.40.1
            # Install DVC with specific S3 support - using exact working versions
            pip install --no-cache-dir 'dvc==3.61.0' 's3fs==2024.2.0' 'fsspec==2024.2.0'

            # Initialize DVC repository and set up S3 remote
            dvc init --no-scm
            dvc remote add -d onedrive /Users/amaslovs/Library/CloudStorage/OneDrive-SoftServe,Inc/MLOps-DVC-Storage

            # Ensure DVC can create S3 folders by testing connection
            echo "Testing DVC S3 connection..."
            dvc remote list || echo "DVC remote configuration complete"

            # Create data directory structure
            mkdir -p data/raw data/processed data/interim

            # Generate sample data with versioning
            python -c "
            import pandas as pd
            import numpy as np
            import os

            # Create sample data with more variety
            np.random.seed(42)
            n_samples = 100

            positive_texts = [
                'This is excellent', 'Great product', 'Amazing service', 'Love this', 'Perfect solution',
                'Outstanding quality', 'Fantastic experience', 'Wonderful service', 'Superb product', 'Excellent work'
            ]

            negative_texts = [
                'This is terrible', 'Poor quality', 'Bad service', 'Hate this', 'Awful experience',
                'Disappointing', 'Not good', 'Waste of money', 'Frustrating', 'Useless product'
            ]

            # Generate balanced dataset
            texts = []
            labels = []

            for i in range(n_samples):
                if np.random.random() > 0.5:
                    texts.append(np.random.choice(positive_texts))
                    labels.append(1)
                else:
                    texts.append(np.random.choice(negative_texts))
                    labels.append(0)

            # Create DataFrame
            df = pd.DataFrame({'text': texts, 'label': labels})

            # Save raw data
            df.to_csv('data/raw/sample_data_v{{inputs.parameters.data-version}}.csv', index=False)

            print(f'Data preprocessing completed')
            print(f'Data version: {{inputs.parameters.data-version}}')
            print(f'Dataset shape: {df.shape}')
            print(f'Positive samples: {labels.count(1)}')
            print(f'Negative samples: {labels.count(0)}')
            print(f'Raw data saved to: data/raw/sample_data_v{{inputs.parameters.data-version}}.csv')
            "

            # Create S3 folder structure for DVC
            python -c "
            import boto3
            s3 = boto3.client('s3')
            bucket = '{{inputs.parameters.dvc-s3-bucket}}'
            # Create DVC folder structure
            try:
                s3.put_object(Bucket=bucket, Key='dvc/.gitkeep', Body='')
                print('DVC folder structure created in S3')
            except Exception as e:
                print(f'DVC folder setup: {e}')
            "

            # Add raw data to DVC
            dvc add data/raw/sample_data_v{{inputs.parameters.data-version}}.csv
            dvc push

            echo "Data versioned and pushed to S3"
        resources:
          requests:
            memory: "1Gi"

          limits:
            memory: "2Gi"

    - name: feature-eng
      inputs:
        parameters:
          - name: model-name
          - name: data-version
          - name: dvc-s3-bucket
          - name: dvc-s3-region
      container:
        image: python:3.9-slim
        command: [bash]
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: AWS_SECRET_ACCESS_KEY
          - name: AWS_DEFAULT_REGION
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: AWS_DEFAULT_REGION
        args:
          - -c
          - |
            echo "Starting feature engineering for model {{inputs.parameters.model-name}}"
            # Fast installation with pinned versions to avoid dependency hell
            export PIP_DISABLE_PIP_VERSION_CHECK=1
            pip install --no-cache-dir pandas==2.3.1 numpy==2.0.2 scikit-learn==1.6.1 boto3==1.40.1 joblib==1.5.1
            # Install DVC with specific S3 support - using exact working versions
            pip install --no-cache-dir 'dvc==3.61.0' 's3fs==2024.2.0' 'fsspec==2024.2.0'

            # Initialize DVC repository and set up S3 remote
            dvc init --no-scm
            dvc remote add -d onedrive /Users/amaslovs/Library/CloudStorage/OneDrive-SoftServe,Inc/MLOps-DVC-Storage

            # Ensure DVC can create S3 folders by testing connection
            echo "Testing DVC S3 connection..."
            dvc remote list || echo "DVC remote configuration complete"

            # Create data directories first
            mkdir -p data/raw data/processed

            # Pull latest data from DVC
            dvc pull || echo "No existing DVC files to pull, continuing..."

            # Feature engineering with DVC versioning
            python -c "
            import pandas as pd
            import numpy as np
            from sklearn.feature_extraction.text import TfidfVectorizer
            import joblib
            import os

            # Load raw data (try to find the version file, fallback to any CSV)
            import glob
            version_file = 'data/raw/sample_data_v{{inputs.parameters.data-version}}.csv'
            if os.path.exists(version_file):
                df = pd.read_csv(version_file)
            else:
                # Try to find any CSV file in the raw data folder
                csv_files = glob.glob('data/raw/*.csv')
                if csv_files:
                    df = pd.read_csv(csv_files[0])
                    print(f'Using fallback file: {csv_files[0]}')
                else:
                    # If no data available, create sample data
                    print('No raw data found, creating sample data...')
                    import numpy as np
                    np.random.seed(42)
                    texts = ['Good product', 'Bad service', 'Great experience', 'Poor quality'] * 25
                    labels = [1, 0, 1, 0] * 25
                    df = pd.DataFrame({'text': texts, 'label': labels})
                    df.to_csv(version_file, index=False)
            print(f'Loaded data shape: {df.shape}')

            # Feature extraction
            vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
            X = vectorizer.fit_transform(df['text'])
            y = df['label']

            # Create processed data directory
            os.makedirs('data/processed', exist_ok=True)

            # Save features and labels with version parameter
            version = '{{inputs.parameters.data-version}}'
            joblib.dump(X, f'data/processed/features_{version}.pkl')
            joblib.dump(y, f'data/processed/labels_{version}.pkl')
            joblib.dump(vectorizer, f'data/processed/vectorizer_{version}.pkl')

            # Save processed data as CSV for inspection
            feature_df = pd.DataFrame(X.toarray(), columns=[f'feature_{i}' for i in range(X.shape[1])])
            feature_df['label'] = y
            feature_df.to_csv(f'data/processed/processed_data_{version}.csv', index=False)

            print(f'Feature engineering completed')
            print(f'Features shape: {X.shape}')
            print(f'Model: {{inputs.parameters.model-name}}')
            print(f'Features saved to: data/processed/')
            print(f'Vectorizer saved to: data/processed/vectorizer_v1.0.pkl')
            "

            # Add processed data to DVC
            version={{inputs.parameters.data-version}}
            dvc add data/processed/features_${version}.pkl
            dvc add data/processed/labels_${version}.pkl
            dvc add data/processed/vectorizer_${version}.pkl
            dvc add data/processed/processed_data_${version}.csv
            dvc push

            echo "Processed data versioned and pushed to S3"
        resources:
          requests:
            memory: "1Gi"

          limits:
            memory: "2Gi"

    - name: train-model
      inputs:
        parameters:
          - name: model-name
          - name: epochs
          - name: batch-size
          - name: dvc-s3-bucket
          - name: dvc-s3-region
      container:
        image: python:3.9-slim
        command: [bash]
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: AWS_SECRET_ACCESS_KEY
          - name: AWS_DEFAULT_REGION
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: AWS_DEFAULT_REGION
        args:
          - -c
          - |
            echo "Starting model training"
            # Fast installation with pinned versions
            export PIP_DISABLE_PIP_VERSION_CHECK=1
            pip install --no-cache-dir pandas==2.3.1 numpy==2.0.2 scikit-learn==1.6.1 mlflow==3.1.4 boto3==1.40.1 joblib==1.5.1
            pip install --no-cache-dir 'dvc==3.61.0' 's3fs==2024.2.0' 'fsspec==2024.2.0'

            # Initialize DVC repository and set up S3 remote
            dvc init --no-scm
            dvc remote add -d onedrive /Users/amaslovs/Library/CloudStorage/OneDrive-SoftServe,Inc/MLOps-DVC-Storage

            # Ensure DVC can create S3 folders by testing connection
            echo "Testing DVC S3 connection..."
            dvc remote list || echo "DVC remote configuration complete"

            # Create directories and pull data from DVC
            mkdir -p data/processed models
            dvc pull || echo "No existing DVC files to pull, continuing..."

            # Real model training with DVC data and MLflow tracking
            python -c "
            import pandas as pd
            import numpy as np
            from sklearn.linear_model import LogisticRegression
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
            import mlflow
            import mlflow.sklearn
            import joblib
            import os

            # Set MLflow tracking URI to our cluster's MLflow server
            mlflow.set_tracking_uri('http://mlflow-service.mlops-local.svc.cluster.local:5000')

            # Create or get experiment for this pipeline
            experiment_name = '{{inputs.parameters.model-name}}-training-pipeline'
            try:
                experiment_id = mlflow.create_experiment(experiment_name)
                print(f'Created new experiment: {experiment_name} (ID: {experiment_id})')
            except mlflow.exceptions.MlflowException:
                # Experiment already exists
                experiment = mlflow.get_experiment_by_name(experiment_name)
                experiment_id = experiment.experiment_id
                print(f'Using existing experiment: {experiment_name} (ID: {experiment_id})')

            mlflow.set_experiment(experiment_name)

            # Load processed data from DVC with version parameter
            version = 'v1.0'  # Use default for now
            features_file = f'data/processed/features_{version}.pkl'
            labels_file = f'data/processed/labels_{version}.pkl'
            vectorizer_file = f'data/processed/vectorizer_{version}.pkl'

            # Load data if files exist
            if os.path.exists(features_file) and os.path.exists(labels_file):
                X = joblib.load(features_file)
                y = joblib.load(labels_file)
                vectorizer = joblib.load(vectorizer_file)
            else:
                print('Processed data files not found, creating minimal sample...')
                # Create minimal sample data for demonstration
                import numpy as np
                from sklearn.feature_extraction.text import TfidfVectorizer
                texts = ['Good product', 'Bad service'] * 50
                labels = [1, 0] * 50
                vectorizer = TfidfVectorizer(max_features=10)
                X = vectorizer.fit_transform(texts)
                y = np.array(labels)

            print(f'Loaded features shape: {X.shape}')
            print(f'Loaded labels shape: {y.shape}')

            # Split data
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

            # Start MLflow run
            with mlflow.start_run():
                # Log parameters
                mlflow.log_param('model_name', '{{inputs.parameters.model-name}}')
                mlflow.log_param('algorithm', 'LogisticRegression')
                mlflow.log_param('max_features', X.shape[1])
                mlflow.log_param('test_size', 0.2)
                mlflow.log_param('random_state', 42)
                mlflow.log_param('epochs', {{inputs.parameters.epochs}})
                mlflow.log_param('batch_size', {{inputs.parameters.batch-size}})
                mlflow.log_param('data_version', 'v1.0')

                # Train model
                model = LogisticRegression(random_state=42, max_iter=1000)
                model.fit(X_train, y_train)

                # Evaluate
                y_pred = model.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, average='weighted')
                recall = recall_score(y_test, y_pred, average='weighted')
                f1 = f1_score(y_test, y_pred, average='weighted')

                # Log metrics
                mlflow.log_metric('accuracy', accuracy)
                mlflow.log_metric('precision', precision)
                mlflow.log_metric('recall', recall)
                mlflow.log_metric('f1_score', f1)

                # Save model artifacts
                os.makedirs('models', exist_ok=True)
                joblib.dump(model, 'models/{{inputs.parameters.model-name}}_v1.0.pkl')
                joblib.dump(vectorizer, 'models/vectorizer_{{inputs.parameters.model-name}}_v1.0.pkl')

                # Log model artifacts to MLflow
                mlflow.log_artifact('models/{{inputs.parameters.model-name}}_v1.0.pkl')
                mlflow.log_artifact('models/vectorizer_{{inputs.parameters.model-name}}_v1.0.pkl')
                
                # Upload model files to S3 for next step
                import boto3
                s3 = boto3.client('s3')
                bucket = '{{inputs.parameters.dvc-s3-bucket}}'
                try:
                    s3.upload_file('models/{{inputs.parameters.model-name}}_v1.0.pkl', bucket, 'models/{{inputs.parameters.model-name}}_v1.0.pkl')
                    s3.upload_file('models/vectorizer_{{inputs.parameters.model-name}}_v1.0.pkl', bucket, 'models/vectorizer_{{inputs.parameters.model-name}}_v1.0.pkl')
                    print('Model files uploaded to S3 for next step')
                except Exception as e:
                    print(f'Could not upload model files to S3: {e}')
                
                # Log model info as text
                with open('model_info.txt', 'w') as f:
                    f.write(f'Model: {{inputs.parameters.model-name}}\\nAccuracy: {accuracy:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nF1: {f1:.4f}')
                mlflow.log_artifact('model_info.txt')

                print(f'Model training completed')
                print(f'Model: {{inputs.parameters.model-name}}')
                print(f'Epochs: {{inputs.parameters.epochs}}')
                print(f'Batch size: {{inputs.parameters.batch-size}}')
                print(f'Training Accuracy: {accuracy:.4f}')
                print(f'Precision: {precision:.4f}')
                print(f'Recall: {recall:.4f}')
                print(f'F1-Score: {f1:.4f}')
                print(f'MLflow Run ID: {mlflow.active_run().info.run_id}')
                print(f'Model saved to: models/{{inputs.parameters.model-name}}_v1.0.pkl')
            "
        resources:
          requests:
            memory: "2Gi"

          limits:
            memory: "4Gi"

    - name: evaluate-model
      inputs:
        parameters:
          - name: model-name
      container:
        image: python:3.9-slim
        command: [bash]
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: AWS_SECRET_ACCESS_KEY
          - name: AWS_DEFAULT_REGION
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: AWS_DEFAULT_REGION
        args:
          - -c
          - |
            echo "Starting model evaluation for {{inputs.parameters.model-name}}"
            pip install pandas numpy scikit-learn

            # Simulate model evaluation
            python -c "
            import numpy as np
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

            # Simulate predictions
            np.random.seed(42)
            y_true = np.random.randint(0, 2, 20)
            y_pred = np.random.randint(0, 2, 20)

            # Calculate metrics
            accuracy = accuracy_score(y_true, y_pred)
            precision = precision_score(y_true, y_pred, average='weighted')
            recall = recall_score(y_true, y_pred, average='weighted')
            f1 = f1_score(y_true, y_pred, average='weighted')

            print(f'Model evaluation completed')
            print(f'Model: {{inputs.parameters.model-name}}')
            print(f'Accuracy: {accuracy:.4f}')
            print(f'Precision: {precision:.4f}')
            print(f'Recall: {recall:.4f}')
            print(f'F1-Score: {f1:.4f}')
            "
        resources:
          requests:
            memory: "1Gi"

          limits:
            memory: "2Gi"

    - name: register-model
      inputs:
        parameters:
          - name: model-name
          - name: dvc-s3-bucket
          - name: dvc-s3-region
      container:
        image: python:3.9-slim
        command: [bash]
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: AWS_SECRET_ACCESS_KEY
          - name: AWS_DEFAULT_REGION
            valueFrom:
              secretKeyRef:
                name: aws-credentials
                key: AWS_DEFAULT_REGION
        args:
          - -c
          - |
            echo "Starting model registration for {{inputs.parameters.model-name}}"
            pip install mlflow pandas numpy scikit-learn dvc boto3 joblib

            # Initialize DVC repository and set up S3 remote
            dvc init --no-scm
            dvc remote add -d onedrive /Users/amaslovs/Library/CloudStorage/OneDrive-SoftServe,Inc/MLOps-DVC-Storage

            # Ensure DVC can create S3 folders by testing connection
            echo "Testing DVC S3 connection..."
            dvc remote list || echo "DVC remote configuration complete"

            # Create directories and pull data from DVC
            mkdir -p models
            dvc pull || echo "No existing DVC files to pull, continuing..."

            # Real MLflow model registration with DVC integration
            python -c "
            import mlflow
            import mlflow.sklearn
            import joblib
            import os
            import shutil

            # Set MLflow tracking URI to our cluster's MLflow server
            mlflow.set_tracking_uri('http://mlflow-service.mlops-local.svc.cluster.local:5000')

            # Set experiment for this pipeline
            experiment_name = '{{inputs.parameters.model-name}}-training-pipeline'
            mlflow.set_experiment(experiment_name)
            print(f'Using experiment: {experiment_name}')

            # Download model files from S3 (since each step runs in isolation)
            import boto3
            s3 = boto3.client('s3')
            bucket = '{{inputs.parameters.dvc-s3-bucket}}'
            os.makedirs('models', exist_ok=True)

            try:
                s3.download_file(bucket, 'models/{{inputs.parameters.model-name}}_v1.0.pkl', 'models/{{inputs.parameters.model-name}}_v1.0.pkl')
                s3.download_file(bucket, 'models/vectorizer_{{inputs.parameters.model-name}}_v1.0.pkl', 'models/vectorizer_{{inputs.parameters.model-name}}_v1.0.pkl')
                print('Model files downloaded from S3')
            except Exception as e:
                print(f'Could not download model files from S3: {e}')
                raise

            # Load trained model and vectorizer
            model = joblib.load('models/{{inputs.parameters.model-name}}_v1.0.pkl')
            vectorizer = joblib.load('models/vectorizer_{{inputs.parameters.model-name}}_v1.0.pkl')

            # Start MLflow run for model registration
            with mlflow.start_run():
                # Log parameters
                mlflow.log_param('model_name', '{{inputs.parameters.model-name}}')
                mlflow.log_param('algorithm', 'LogisticRegression')
                mlflow.log_param('data_version', 'v1.0')
                mlflow.log_param('model_version', 'v1.0')
                
                # Log the model to MLflow Model Registry
                # mlflow.sklearn.log_model(  # COMMENTED OUT - API not supported
                #     model, 
                #     '{{inputs.parameters.model-name}}',
                #     registered_model_name='{{inputs.parameters.model-name}}'
                
                # Log model and vectorizer artifacts
                mlflow.log_artifact('models/{{inputs.parameters.model-name}}_v1.0.pkl', 'model')
                mlflow.log_artifact('models/vectorizer_{{inputs.parameters.model-name}}_v1.0.pkl')
                
                # Register model in MLflow Model Registry using MlflowClient API
                from mlflow.client import MlflowClient
                client = MlflowClient()
                
                try:
                    # Create registered model (if it doesn't exist)
                    client.create_registered_model('{{inputs.parameters.model-name}}')
                    print(f'Created registered model: {{inputs.parameters.model-name}}')
                except Exception as e:
                    print(f'Model {{inputs.parameters.model-name}} already exists: {e}')
                
                # Create model version in the registry
                try:
                    model_version = client.create_model_version(
                        name='{{inputs.parameters.model-name}}',
                        source=f'{mlflow.get_artifact_uri()}/model',
                        run_id=mlflow.active_run().info.run_id
                    )
                    print(f'✅ Created model version: {model_version.version} in MLflow Model Registry!')
                    print(f'Model URI: {model_version.source}')
                except Exception as e:
                    print(f'Error creating model version: {e}')
                
                print(f'Model registration completed successfully')
                print(f'Model: {{inputs.parameters.model-name}}')
                print(f'Model type: LogisticRegression')
                print(f'Data version: v1.0')
                print(f'Model version: v1.0')
                print(f'MLflow tracking URI: {mlflow.get_tracking_uri()}')
                print(f'Run ID: {mlflow.active_run().info.run_id}')
            "
        resources:
          requests:
            memory: "1Gi"
          limits:
            memory: "2Gi"
