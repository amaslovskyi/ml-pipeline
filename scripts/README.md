# ğŸ“ Scripts Directory Structure

This document outlines the organized structure of the scripts directory for better maintainability and clear separation of concerns.

## ğŸ“‚ Directory Overview

```
scripts/
â”œâ”€â”€ aws-tools/           # AWS infrastructure and credential management
â”œâ”€â”€ argo-tools/         # Argo Workflows and GitOps tools
â”œâ”€â”€ data-tools/         # Data processing and DVC management
â”œâ”€â”€ deployment/         # Container and Kubernetes deployment files
â”œâ”€â”€ evaluation-legacy/  # Legacy evaluation scripts (moved to eval/)
â”œâ”€â”€ inference-legacy/   # Legacy inference tools (deprecated)
â””â”€â”€ ollama-legacy/     # Legacy Ollama conversion attempts
```

## â˜ï¸ aws-tools/

**Purpose**: AWS infrastructure setup and credential management

| **File**                    | **Purpose**                             | **Usage**                     |
| --------------------------- | --------------------------------------- | ----------------------------- |
| `deploy-s3-bucket.sh`       | Deploy S3 bucket for MLOps data storage | `./deploy-s3-bucket.sh`       |
| `update-aws-credentials.sh` | Update AWS credentials configuration    | `./update-aws-credentials.sh` |
| `setup-credentials.sh`      | Initial AWS credentials setup           | `./setup-credentials.sh`      |

## ğŸ”„ argo-tools/

**Purpose**: Argo Workflows and GitOps pipeline management

| **File**                           | **Purpose**                           | **Usage**                            |
| ---------------------------------- | ------------------------------------- | ------------------------------------ |
| `setup-argo.sh`                    | Install and configure Argo Workflows  | `./setup-argo.sh`                    |
| `start-argo-ui.sh`                 | Start Argo UI for workflow monitoring | `./start-argo-ui.sh`                 |
| `run-argo-pipeline.sh`             | Execute ML training pipeline          | `./run-argo-pipeline.sh`             |
| `run-git-workflow.sh`              | Run Git-based workflow                | `./run-git-workflow.sh`              |
| `setup-repo-connection.sh`         | Configure repository connections      | `./setup-repo-connection.sh`         |
| `validate-deployment-readiness.sh` | Check deployment prerequisites        | `./validate-deployment-readiness.sh` |

## ğŸ“Š data-tools/

**Purpose**: Data processing, versioning, and DVC management

| **File**                             | **Purpose**                             | **Usage**                                   |
| ------------------------------------ | --------------------------------------- | ------------------------------------------- |
| `prepare-enhanced-devops-dataset.py` | Create enhanced DevOps training dataset | `python prepare-enhanced-devops-dataset.py` |
| `test-enhanced-dataset.py`           | Validate enhanced dataset quality       | `python test-enhanced-dataset.py`           |
| `update-dvc-config.sh`               | Update DVC configuration                | `./update-dvc-config.sh`                    |
| `update-dvc-to-onedrive.sh`          | Configure DVC for OneDrive storage      | `./update-dvc-to-onedrive.sh`               |

## ğŸš€ deployment/

**Purpose**: Container and Kubernetes deployment configurations

| **File**                     | **Purpose**                         | **Usage**                                   |
| ---------------------------- | ----------------------------------- | ------------------------------------------- |
| `Dockerfile`                 | Container image for model inference | `docker build -f Dockerfile .`              |
| `k8s-deployment.yaml`        | Kubernetes deployment manifest      | `kubectl apply -f k8s-deployment.yaml`      |
| `deploy-k8s.sh`              | Deploy to Kubernetes cluster        | `./deploy-k8s.sh`                           |
| `deploy-ollama.sh`           | Deploy Ollama model locally         | `./deploy-ollama.sh`                        |
| `requirements-inference.txt` | Python dependencies for inference   | `pip install -r requirements-inference.txt` |

## âœ… **Clean Structure** 

All legacy directories have been removed:
- âŒ `evaluation-legacy/` - Duplicates moved to `../eval/evaluation-scripts/`
- âŒ `inference-legacy/` - Superseded by `../eval/server-tools/`  
- âŒ `ollama-legacy/` - Failed attempts, working solution in `../eval/ollama-tools/`

## ğŸš€ Quick Start Commands

### Setup AWS Infrastructure
```bash
cd aws-tools/
./setup-credentials.sh
./deploy-s3-bucket.sh
```

### Setup Argo Workflows
```bash
cd argo-tools/
./setup-argo.sh
./start-argo-ui.sh
```

### Prepare Training Data
```bash
cd data-tools/
python prepare-enhanced-devops-dataset.py
python test-enhanced-dataset.py
```

### Deploy Model
```bash
cd deployment/
./deploy-k8s.sh
# OR for local Ollama deployment
./deploy-ollama.sh
```

## ğŸ”§ Dependencies

- **aws-tools/**: AWS CLI, proper AWS credentials
- **argo-tools/**: kubectl, Argo Workflows CLI
- **data-tools/**: Python 3.8+, DVC, pandas, datasets
- **deployment/**: Docker, kubectl (for K8s), ollama (for local)

## ğŸ“ Migration Notes

- **Evaluation scripts**: Moved to `../eval/evaluation-scripts/`
- **Server tools**: Moved to `../eval/server-tools/`
- **Ollama tools**: Working versions moved to `../eval/ollama-tools/`

## ğŸ’¾ **Space Saved**

Cleanup results:
- **~15GB freed** by removing duplicate model files
- **Reduced complexity** with clear, focused directory structure
- **No functionality lost** - all working tools preserved in `../eval/`

---

**ğŸ¯ Remember**: This organized structure separates production tools from legacy code and groups related functionality for better maintainability.
